## Objective: Generate a merged product that uses the method described in Soviadan et al.(in review) to select the maximum NBSS for a given sample

## Requirements: Computation of group-specific NBSS (Step 5)

## Python modules and path specifications:
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
from pathlib import Path
import statistics as st
import os
import datetime, time
from natsort import natsorted
import shutil
from tqdm import tqdm
from functools import reduce
try:
    from funcs_NBS import *
    from funcs_standardize_projects import *
    from funcs_merged_products import *

except:
    from scripts.funcs_NBS import *
    from scripts.funcs_standardize_projects import *
    from scripts.funcs_merged_products import *

## Workflow starts here:

# Load most recent NBSS taxa-specific products
path_to_products=natsorted(list((sorted(list(Path(path_to_git / cfg['dataset_subdir'] /cfg['product_subdir']).glob('NBSS_ver_*')),key = lambda fname:time.strptime(os.path.basename(fname),"NBSS_ver_%m_%Y"))[-1]).rglob('PFT/*-distribution_all_var_*'))) #rglob('PFT/*_1a_*')
grouping_factor=['date_bin','Station_location','midLatBin','midLonBin']#['ocean','year','month','latitude','longitude']
path_to_directory=Path(path_to_products[0].parent.parent / 'Merged')
path_to_directory.mkdir(exist_ok=True)
currentYear,currentMonth=path_to_directory.parent.stem.split("_")[-1],path_to_directory.parent.stem.split("_")[-2]
with tqdm(desc='Merging taxa-specific {} products', total=3, bar_format='{desc}{bar}', position=0, leave=True) as bar:
    for product in ['Size','Biomass','Weight']:
        bar.set_description("Merging taxa-specific {} products. Generating the products for multi-instrument bins".format(product), refresh=True)
        # Select the associated files
        path_to_files=[path for path in path_to_products if product in  path.name]
        df_nbss=merge_products(path_to_products=path_to_files, grouping_factors=grouping_factor)
        # SPlit multi-instruments versus single-instrument bins
        df,df_single=  df_nbss[df_nbss.n_instruments > 1],df_nbss[df_nbss.n_instruments == 1]
        group = grouping_factor
        df,df_single = pd.merge(df, df.drop_duplicates(subset=group, ignore_index=True)[group].reset_index().rename({'index': 'Group_station_index'}, axis='columns'), how='left', on=group),pd.merge(df_single, df_single.drop_duplicates(subset=group, ignore_index=True)[group].reset_index().rename({'index': 'Group_station_index'}, axis='columns'), how='left', on=group)
        df,df_single =df.assign(Sample=df.midLatBin.astype(str) + '_' + df.midLonBin.astype(str) + "_" + df.date_bin.astype( str)).rename(columns={'midLonBin': 'Longitude', 'midLatBin': 'Latitude','Min_obs_depth':'min_depth','Max_obs_depth':'max_depth'}),df_single.assign(Sample=df_single.midLatBin.astype(str) + '_' + df_single.midLonBin.astype(str) + "_" + df_single.date_bin.astype( str)).rename(columns={'midLonBin': 'Longitude', 'midLatBin': 'Latitude','Min_obs_depth':'min_depth','Max_obs_depth':'max_depth'}) #df.assign(Sample=df.latitude.astype(str) + '_' + df.longitude.astype(str) + "_" + df.year.astype( str) + df.month.astype(str).str.zfill(2)).rename(columns={'longitude': 'Longitude', 'latitude': 'Latitude'})
        # Use class mid-point (biovolume, biomass, or weight) to assign the rest of the size classes information
        #data_bins =df_bins.drop(columns=['sizeClasses']).assign(sizeClasses=pd.cut(np.append(0,df_bins[dict_products_x[product]].to_numpy()),np.append(0,df_bins[dict_products_x[product]].to_numpy())).categories.values.astype(str))# df_bins.drop(columns=['sizeClasses']).assign(sizeClasses=pd.cut(np.append(0,df_bins[dict_products_x[product]].to_numpy()),np.append(0,df_bins[dict_products_x[product]].to_numpy())).categories.values.astype(str))
        df,df_single=pd.merge(df.drop(columns=[column for column in df.columns if column in df_bins.columns]).assign(sizeClasses=pd.cut(df[dict_products_x[product]],data_bins[dict_products_x[product]]).astype(str)),df_bins.drop(columns=['sizeClasses']).assign(sizeClasses=pd.cut(data_bins[dict_products_x[product]].to_numpy(),data_bins[dict_products_x[product]].to_numpy()).categories.values.astype(str)),how='left',on='sizeClasses'),pd.merge(df_single.drop(columns=[column for column in df_single.columns if column in df_bins.columns]).assign(sizeClasses=pd.cut(df_single[dict_products_x[product]],data_bins[dict_products_x[product]]).astype(str)),df_bins.drop(columns=['sizeClasses']).assign(sizeClasses=pd.cut(data_bins[dict_products_x[product]].to_numpy(),data_bins[dict_products_x[product]].to_numpy()).categories.values.astype(str)),how='left',on='sizeClasses') # data_bins
        # Original data is non-thresholded so apply a msk to discard biased estimates
        df.loc[(df.count_uncertainty>cfg['artefacts_threshold']) | (df.size_uncertainty>cfg['artefacts_threshold']),dict_products_y[product]]=np.nan
        df_single.loc[(df_single.count_uncertainty > cfg['artefacts_threshold']) | (df_single.size_uncertainty > cfg['artefacts_threshold']),dict_products_y[product]] = np.nan

        ## Apply Soviadan et al. maximum selection method: https://doi.org/10.1101/2023.06.29.547051
        # Use the maximum normalized abundance / biovolume / biomass / weight in each size class
        #x=df.loc[list(df.groupby(group+['PFT','sizeClasses']).groups.values())[3]]
        group=['Latitude', 'Longitude', 'date_bin','Group_station_index','n_instruments','merged_instruments']#['Latitude', 'Longitude', 'year', 'month','Group_station_index','n_instruments','merged_instruments']
        df_selection=df.dropna(subset=dict_products_y[product]).reset_index(drop=True).groupby(group+['PFT','sizeClasses']).apply(lambda x: pd.Series({**{'Validation_percentage':df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].Validation_percentage.mean(),'min_depth':df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.min(),'max_depth':df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.max(),'selected_instrument':(x.loc[(x[dict_products_y[product]]).idxmax(axis=0),'Instrument']).values[0]},**dict(zip(dict_products_y[product], [(x[dict_products_y[product]]).max(axis=0).values[0]]))}) if any((x[dict_products_y[product]].isna()==False).to_numpy()) else None).reset_index()#df.groupby(group+['PFT','sizeClasses']).apply(lambda x: pd.Series({**{'min_depth':df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.min(),'max_depth':df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.max(),'selected_instrument':x.loc[(x[dict_products_y[product]]*np.repeat(np.where((x.max_depth.astype(float) - x.min_depth.astype(float)) > 1,(x.max_depth.astype(float) - x.min_depth.astype(float)), 1) / np.where( (df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()) > 1,(df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()), 1),len(dict_products_y[product])).reshape(len(x),len(dict_products_y[product]))).idxmax(axis=0),'Instrument'].unique()[0]},**dict(zip(dict_products_y[product], ( x[dict_products_y[product]]*np.repeat(np.where((x.max_depth.astype(float) - x.min_depth.astype(float)) > 1,(x.max_depth.astype(float) - x.min_depth.astype(float)), 1) / np.where( (df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()) > 1,(df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()), 1),len(dict_products_y[product])).reshape(len(x),len(dict_products_y[product]))).max(axis=0))),**dict(zip([variable.replace('_mean','_std') for variable in dict_products_y[product]],np.diag(x.loc[(x[dict_products_y[product]]*np.repeat(np.where((x.max_depth.astype(float) - x.min_depth.astype(float)) > 1,(x.max_depth.astype(float) - x.min_depth.astype(float)), 1) / np.where( (df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()) > 1,(df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()), 1),len(dict_products_y[product])).reshape(len(x),len(dict_products_y[product]))).idxmax(axis=0),[variable.replace('_mean','_std') for variable in dict_products_y[product]]]))),**{'n':x.loc[(x[dict_products_y[product]]*np.repeat(np.where((x.max_depth.astype(float) - x.min_depth.astype(float)) > 1,(x.max_depth.astype(float) - x.min_depth.astype(float)), 1) / np.where( (df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()) > 1,(df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()), 1),len(dict_products_y[product])).reshape(len(x),len(dict_products_y[product]))).idxmax(axis=0),'n'].unique()[0]}})).reset_index()#df.groupby(group+['PFT','sizeClasses']).apply(lambda x: pd.Series({**{'Validation_percentage':df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].Validation_percentage.mean(),'min_depth':df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.min(),'max_depth':df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.max(),'selected_instrument':x.loc[(x[dict_products_y[product]]).idxmax(axis=0),'Instrument'].unique()[0]},**dict(zip(dict_products_y[product], (x[dict_products_y[product]]).max(axis=0))),**dict(zip([variable.replace('_mean','_std') for variable in dict_products_y[product]],np.diag(x.loc[(x[dict_products_y[product]]).idxmax(axis=0),[variable.replace('_mean','_std') for variable in dict_products_y[product]]]))),**{'n':x.loc[(x[dict_products_y[product]]).idxmax(axis=0),'n'].unique()[0]}})).reset_index()#df.groupby(group+['PFT','sizeClasses']).apply(lambda x: pd.Series({**{'min_depth':df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.min(),'max_depth':df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.max(),'selected_instrument':x.loc[(x[dict_products_y[product]]*np.repeat(np.where((x.max_depth.astype(float) - x.min_depth.astype(float)) > 1,(x.max_depth.astype(float) - x.min_depth.astype(float)), 1) / np.where( (df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()) > 1,(df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()), 1),len(dict_products_y[product])).reshape(len(x),len(dict_products_y[product]))).idxmax(axis=0),'Instrument'].unique()[0]},**dict(zip(dict_products_y[product], ( x[dict_products_y[product]]*np.repeat(np.where((x.max_depth.astype(float) - x.min_depth.astype(float)) > 1,(x.max_depth.astype(float) - x.min_depth.astype(float)), 1) / np.where( (df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()) > 1,(df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()), 1),len(dict_products_y[product])).reshape(len(x),len(dict_products_y[product]))).max(axis=0))),**dict(zip([variable.replace('_mean','_std') for variable in dict_products_y[product]],np.diag(x.loc[(x[dict_products_y[product]]*np.repeat(np.where((x.max_depth.astype(float) - x.min_depth.astype(float)) > 1,(x.max_depth.astype(float) - x.min_depth.astype(float)), 1) / np.where( (df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()) > 1,(df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()), 1),len(dict_products_y[product])).reshape(len(x),len(dict_products_y[product]))).idxmax(axis=0),[variable.replace('_mean','_std') for variable in dict_products_y[product]]]))),**{'n':x.loc[(x[dict_products_y[product]]*np.repeat(np.where((x.max_depth.astype(float) - x.min_depth.astype(float)) > 1,(x.max_depth.astype(float) - x.min_depth.astype(float)), 1) / np.where( (df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()) > 1,(df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.astype(float).max() - df[df[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.astype(float).min()), 1),len(dict_products_y[product])).reshape(len(x),len(dict_products_y[product]))).idxmax(axis=0),'n'].unique()[0]}})).reset_index()
        df_selection=df_selection.dropna(subset='selected_instrument').reset_index(drop=True)
        # Plot the selection for each PFT
        if product=='Size':
            # Taxa-specific selection
            plot_data=pd.merge(df_selection,df_bins.drop(columns=['sizeClasses']).assign(sizeClasses=pd.cut(data_bins[dict_products_x[product]].to_numpy(),data_bins[dict_products_x[product]].to_numpy()).categories.values.astype(str)),how='left',on='sizeClasses').sort_values(['Latitude', 'Longitude', 'Group_station_index','PFT','ECD_mid']).reset_index(drop=True).assign(latitude_bin=lambda x:pd.cut(x.Latitude.astype(float),bins=np.arange(-90,90,1),labels=np.arange(-90,90,1)[1:]).astype(float),PFT=lambda x: pd.Categorical(x.PFT.str.replace("_"," "),['Nanophytoplankton','Microphytoplankton','Mesophytoplankton','Ciliophora','Rhizaria','Crustaceans','Filter feeder gelatinous','Carnivorous gelatinous','Mollusca','Actinopterygii','Other zooplankton','Detritus']))
            plot = (ggplot(data=plot_data) +
                    facet_wrap('~PFT', nrow=2) +
                    plotnine.stat_bin_2d( mapping=aes(x="ECD_mid", y="latitude_bin",group='latitude_bin', fill='selected_instrument'), bins=25,geom='point', shape='H', color='#{:02x}{:02x}{:02x}{:02x}'.format(0, 0, 0, 50)).to_layer() +
                    stat_summary(aes(x="ECD_mid", y="latitude_bin",group='latitude_bin', fill='selected_instrument'),geom='point', size=1.5, shape='H', color='#{:02x}{:02x}{:02x}{:02x}'.format(0, 0, 0, 50), alpha=0.1) +
                    scale_fill_manual(pal_instrument,limits=df_selection.selected_instrument.unique())+
                    labs(fill='Selected instruments:',x=r'Equivalent circular diameter ($\mu$m)', y=r'Latitude ($\degree$N)') +
                    scale_x_log10(breaks=[size  for size in np.sort( np.concatenate(np.arange(1, 10).reshape((9, 1)) * np.power(10, np.arange(1, 5, 1))))],labels=lambda l: [int(size) if (size / np.power(10, np.ceil(np.log10(size)))) == 1 else '' for size in l])+
                    theme_paper).draw(show=True)
            plot.set_size_inches(8.5, 3.8)
            plot.savefig(fname='{}/PSSdb_Merged-products_selection_taxa.svg'.format(str(path_to_directory)), dpi=300)
            # Merged(summed) selection
            plot = (ggplot(data=plot_data.astype({'ECD_mid':str,'latitude_bin':str}).groupby(['Latitude','Longitude','date_bin','Group_station_index','latitude_bin','ECD_mid']).apply(lambda x:pd.Series({'selected_instrument':x.selected_instrument.value_counts(sort=True).index[0],dict_products_y[product][0]:x[dict_products_y[product][0]].sum()})).reset_index().astype({'ECD_mid':float,'latitude_bin':float}).sort_values(['Latitude','Longitude','date_bin','Group_station_index','ECD_mid'])) +
                    #plotnine.stat_bin_2d( mapping=aes(x="ECD_mid", y="latitude_bin",group='factor(latitude_bin)', fill='selected_instrument'), bins=25,geom='point', shape='H', color='#{:02x}{:02x}{:02x}{:02x}'.format(0, 0, 0, 50)).to_layer() +
                    stat_summary(aes(x="ECD_mid", y="latitude_bin",group='latitude_bin', fill='selected_instrument'),geom='point', size=1.5, shape='H', color='#{:02x}{:02x}{:02x}{:02x}'.format(0, 0, 0, 50), alpha=0.1) +
                    scale_fill_manual(pal_instrument,limits=df_selection.selected_instrument.unique())+
                    labs(fill='Selected instruments:',x=r'Equivalent circular diameter ($\mu$m)', y=r'Latitude ($\degree$N)') +
                    scale_x_log10(breaks=[size  for size in np.sort( np.concatenate(np.arange(1, 10).reshape((9, 1)) * np.power(10, np.arange(1, 5, 1))))],labels=lambda l: [int(size) if (size / np.power(10, np.ceil(np.log10(size)))) == 1 else '' for size in l])+
                    theme_paper).draw(show=True)
            plot.set_size_inches(8.5, 1.8)
            plot.savefig(fname='{}/PSSdb_Merged-products_selection_total.svg'.format(str(path_to_directory)), dpi=300)


            df_instruments=df.assign(latitude_bin=lambda x: pd.cut(x.Latitude.astype(float), bins=np.arange(-90, 90, 1), labels=np.arange(-90, 90, 1)[1:]).astype(float),PFT=lambda x: pd.Categorical(x.PFT.str.replace("_", " "), ['Nanophytoplankton', 'Microphytoplankton', 'Mesophytoplankton', 'Ciliophora','Rhizaria', 'Crustaceans', 'Filter feeder gelatinous','Carnivorous gelatinous', 'Mollusca', 'Actinopterygii', 'Other zooplankton','Detritus'])).groupby(['latitude_bin','ECD_mid']).apply(lambda x: pd.Series(dict(zip(pd.Categorical(x.Instrument,df.Instrument.unique()).value_counts().index,(pd.Categorical(x.Instrument,df.Instrument.unique()).value_counts().values>=1).astype(int))))).reset_index().reset_index()
            plot = (ggplot(data=df_instruments.melt(id_vars=['latitude_bin','ECD_mid'],value_vars=df.Instrument.unique(),var_name='Instrument',value_name='value')) +
                    stat_summary(aes(x="ECD_mid", y="Instrument", fill='value>0'),geom='point', size=3.5, shape='H', color='#{:02x}{:02x}{:02x}{:02x}'.format(0, 0, 0, 50), alpha=1) +
                    scale_fill_manual(values={True:'black',False:'#{:02x}{:02x}{:02x}{:02x}'.format(0, 0, 0, 50)},guide=False)+
                    labs(fill='Selected instruments:', x=r'Equivalent circular diameter ($\mu$m)', y=r'') +
                    scale_x_log10(breaks=[size for size in np.sort(np.concatenate(np.arange(1, 10).reshape((9, 1)) * np.power(10, np.arange(1, 5, 1))))],labels=lambda l: [int(size) if (size / np.power(10, np.ceil(np.log10(size)))) == 1 else '' for size in l]) +theme_paper).draw(show=True)
            plot.set_size_inches(8.5, 1.8)
            plot.savefig(fname='{}/PSSdb_Merged-products_selection_Instruments_ECD.svg'.format(str(path_to_directory)), dpi=300)
            plot = (ggplot(data=df_instruments.melt(id_vars=['latitude_bin','ECD_mid'],value_vars=df.Instrument.unique(),var_name='Instrument',value_name='value')) +
                    stat_summary(aes(x="latitude_bin", y="Instrument", fill='value>0'),geom='point', size=2.5, shape='H', color='#{:02x}{:02x}{:02x}{:02x}'.format(0, 0, 0, 50), alpha=1) +
                    coord_flip()+
                    scale_fill_manual(values={True:'black',False:'#{:02x}{:02x}{:02x}{:02x}'.format(0, 0, 0, 50)},guide=False)+
                    labs(fill='Selected instruments:', y=r'', x=r'Latitude ($\degree$N)') +
                    theme_paper+theme(axis_text_x=element_text(rotation=45))).draw(show=True)
            plot.set_size_inches(1.2, 2.9)
            plot.savefig(fname='{}/PSSdb_Merged-products_selection_Instruments_latitude.svg'.format(str(path_to_directory)), dpi=300)

        #x=df_selection.loc[list(df_selection.groupby(['Latitude', 'Longitude', 'year', 'month','min_depth','max_depth','Group_station_index','n_instruments','merged_instruments','sizeClasses']).groups.values())[72]]
        df_merged=df_selection.groupby(['Latitude', 'Longitude', 'date_bin','min_depth','max_depth','Group_station_index','n_instruments','merged_instruments','sizeClasses']).apply(lambda x:pd.Series({**{'Validation_percentage':np.nanmean(x.Validation_percentage)},**dict(zip((pd.Series(dict_products_y[product])+'_living').to_list(),x.query('PFT!="Detritus"')[dict_products_y[product]].sum(axis=0))),**dict(zip((pd.Series(dict_products_y[product])+'_nonliving').to_list(),x.query('PFT=="Detritus"')[dict_products_y[product]].sum(axis=0)))})).reset_index()#df_selection.groupby(['Latitude', 'Longitude', 'year', 'month','min_depth','max_depth','Group_station_index','n_instruments','merged_instruments','sizeClasses']).apply(lambda x:pd.Series({**{'n':x.n.min()},**dict(zip((pd.Series(dict_products_y[product])+'_living').to_list(),x.query('PFT!="Detritus"')[dict_products_y[product]].sum(axis=0))),**dict(zip((pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_living').to_list(),((x.query('PFT!="Detritus"')[(pd.Series(dict_products_y[product]).str.replace('_mean','_std')).to_list()]**2).sum(axis=0))**0.5)),**dict(zip((pd.Series(dict_products_y[product])+'_nonliving').to_list(),x.query('PFT=="Detritus"')[dict_products_y[product]].sum(axis=0))),**dict(zip((pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_nonliving').to_list(),((x.query('PFT=="Detritus"')[(pd.Series(dict_products_y[product]).str.replace('_mean','_std')).to_list()]**2).sum(axis=0))**0.5))})).reset_index()
        df_merged=pd.merge(df_merged,df_bins.drop(columns=['sizeClasses']).assign(sizeClasses=pd.cut(data_bins[dict_products_x[product]].to_numpy(),data_bins[dict_products_x[product]].to_numpy()).categories.values.astype(str)),how='left',on='sizeClasses').sort_values(['Latitude', 'Longitude', 'date_bin','min_depth','max_depth','ECD_mid']).reset_index(drop=True)

        # Check merged products along with original data after generating final product
        df_merged['Sample']=df_merged.Latitude.astype(str) + '_' + df_merged.Longitude.astype(str) + "_" + df_merged.date_bin.astype( str)

        bar.set_description("Merging taxa-specific {} products. Saving all merged mutli-instruments sub-bin products to {}".format(product,str(path_to_directory).replace(str(Path.home()),'~')), refresh=True)
        df_merged.to_csv(path_to_directory / str('Merged_multi-instruments_{}-distribution_all_var_v'.format(product) + currentYear + '-' + currentMonth + '.csv'),index=False)

        ## first, apply thresholding function, then average at bin level
        df_merged_thres=df_merged.groupby(['Sample','Latitude','Longitude','date_bin','min_depth','max_depth','Group_station_index','n_instruments','merged_instruments']).apply(lambda x: (threshold_func(binned_data=x.rename(columns=dict(zip((pd.Series(dict_products_y[product])+'_living').to_list(),(pd.Series(dict_products_y[product])).to_list()))).assign(count_uncertainty=0,size_uncertainty=0), empty_bins=3, threshold_count=0.2, threshold_size=0.2))).reset_index(drop=True)
        df_products_1a=NBSS_stats_func(df=df_merged_thres.rename(columns={'Latitude':'midLatBin','Longitude':'midLonBin','min_depth':'Min_obs_depth','max_depth':'Max_obs_depth'}).astype({'midLatBin':float,'midLonBin':float}).assign(PSD=lambda x:x.PSD if 'PSD' in x.columns else np.nan,size_class_mid=lambda x: x[dict_products_x[product]]), light_parsing=False, bin_loc=1, group_by='yyyymm')
        df_products_1a=pd.merge(df_products_1a,df_merged_thres.assign(biovolume_size_class=lambda x: x[dict_products_x[product]],range_biomass_bin=lambda x:x.range_biomass_bin,latitude=lambda x:pd.cut(x.Latitude.astype(float),bins=np.arange(-90,91,1),labels=np.arange(-90,91,0.5)[1:-1:2]).astype(str),longitude=lambda x:pd.cut(x.Longitude.astype(float),bins=np.arange(-180,181,1),labels=np.arange(-180,181,0.5)[1:-1:2]).astype(str),year=lambda x:(x.date_bin.str[0:4]).astype(str),month=lambda x:(x.date_bin.str.split("_").str[-2]).astype(str)).astype(dict(zip(['latitude','longitude','year','month','biovolume_size_class','range_biomass_bin','min_depth','max_depth','n_instruments','merged_instruments'],[str]*10))).groupby(['latitude','longitude','year','month','biovolume_size_class','range_biomass_bin','min_depth','max_depth']).apply(lambda x: pd.Series({'n_instruments':len(x.merged_instruments.unique()[0].split("_")),'merged_instruments':'_'.join(natsorted(x.merged_instruments.unique()[0].split("_"))),'Validation_percentage':x.Validation_percentage.mean()})).reset_index().astype((df_products_1a.dtypes.loc[['latitude','longitude','year','month','min_depth','max_depth','biovolume_size_class']]).to_dict()),how='left',on=['latitude','longitude','year','month','min_depth','max_depth','biovolume_size_class'])
        df_products_1b=df_merged_thres.rename(columns={'Latitude':'midLatBin','Longitude':'midLonBin','min_depth':'Min_obs_depth','max_depth':'Max_obs_depth'}).astype(dict(zip(['midLatBin','midLonBin','date_bin','Min_obs_depth','Max_obs_depth','n_instruments','merged_instruments'],[str]*7))).groupby(['midLatBin','midLonBin','date_bin','Min_obs_depth','Max_obs_depth','n_instruments','merged_instruments']).apply(lambda x:linear_fit_func(x.astype({'midLatBin':float,'midLonBin':float,'Min_obs_depth':float,'Max_obs_depth':float}).assign(logSize=np.log10(x[dict_products_x[product]]/df_bins.loc[0,dict_products_x[product]]),logNB=np.log10(x[dict_products_y[product][0]]),logECD=np.log10(x.ECD_mid),logPSD=np.log10(x.PSD) if 'PSD' in x.columns else np.nan), light_parsing = False, depth_parsing = False).assign(Validation_percentage=np.nanmean(x.Validation_percentage))).reset_index()
        df_products_1b=pd.merge(stats_linfit_func(df=df_products_1b, light_parsing = False, bin_loc = 1, group_by = 'yyyymm'),df_products_1b.assign(latitude=lambda x:pd.cut(x.midLatBin.astype(float),bins=np.arange(-90,91,1),labels=np.arange(-90,91,0.5)[1:-1:2]).astype(str),longitude=lambda x:pd.cut(x.midLonBin.astype(float),bins=np.arange(-180,181,1),labels=np.arange(-180,181,0.5)[1:-1:2]).astype(str),year=lambda x:(x.date_bin.str[0:4]).astype(str),month=lambda x:(x.date_bin.str.split("_").str[-1]).astype(str)).astype(dict(zip(['latitude','longitude','year','month','Min_obs_depth','Max_obs_depth'],[str]*6))).groupby(['latitude','longitude','year','month','Min_obs_depth','Max_obs_depth']).apply(lambda x: pd.Series({'n_instruments':len(x.merged_instruments.unique()[0].split("_")),'merged_instruments':'_'.join(natsorted(x.merged_instruments.unique()[0].split("_"))),'Validation_percentage':np.nanmean(x.Validation_percentage)})).reset_index().rename(columns={'Min_obs_depth':'min_depth','Max_obs_depth':'max_depth'}).astype({'latitude':float,'longitude':float,'year':str,'month':str,'min_depth':float,'max_depth':float}),how='left',on=['latitude','longitude','year','month','min_depth','max_depth'])
        df_products_merged=pd.merge(df_products_1a,df_products_1b,how='left',on=['merged_instruments','year','month','latitude','longitude','min_depth','max_depth','n'])
        df_products_1a, df_products_1b =ocean_label_func(df=df_products_1a, Lon='longitude', Lat='latitude'),ocean_label_func(df=df_products_1b, Lon='longitude', Lat='latitude')
        df_meta_1a=df_products_1a[['range_biomass_bin','n_instruments','merged_instruments','Validation_percentage']].copy()
        df_meta_1b=df_products_1b[['n_instruments','merged_instruments','Validation_percentage']].copy()

        df_products_1a,df_products_1b=QC_products(df_products_1a,df_products_1b, grouping_factors=['year', 'month', 'latitude', 'longitude', 'ocean'])
        df_products_1a,df_products_1b=pd.concat([df_products_1a,df_meta_1a],axis=1,ignore_index=False),pd.concat([df_products_1b,df_meta_1b],axis=1,ignore_index=False)

        bar.set_description( "Merging taxa-specific {} products. Saving merged mutli-instruments products 1a/b to {}".format(product,str(path_to_directory).replace(str(Path.home()),'~')), refresh = True)
        columns_to_save_1a=list(np.insert(pd.Series(df_products_1a.columns.drop(['n_instruments','merged_instruments','Validation_percentage']).astype(str)).values,[0,0,9],['n_instruments','merged_instruments','Validation_percentage'])) if product=='Size' else list(np.insert(pd.Series(df_products_1a.columns.drop(['equivalent_circular_diameter_mean','normalized_abundance_mean','normalized_abundance_std','range_biomass_bin','n_instruments','merged_instruments','Validation_percentage']).astype(str)).replace({'biovolume_size_class':dict_products_x[product] if product in ['Biomass','Weight'] else 'biovolume_size_class','normalized_biovolume_mean':'normalized_biomass_mean' if product in ['Biomass','Weight'] else 'normalized_biomass_means','normalized_biovolume_std':'normalized_biomass_std' if product in ['Biomass','Weight'] else 'normalized_biovolume_std'}).values,[0,0,9,9],['n_instruments','merged_instruments','range_biomass_bin','Validation_percentage'])) #pd.Series(df_products_1a.columns.drop(['equivalent_circular_diameter_mean','normalized_abundance_mean','normalized_abundance_std']).astype(str)).replace({'biovolume_size_class':dict_products_x[product] if product in ['Biomass','Weight'] else 'biovolume_size_class','normalized_biovolume_mean':'normalized_biomass_mean' if product in ['Biomass','Weight'] else 'normalized_biomass_means','normalized_biovolume_std':'normalized_biomass_std' if product in ['Biomass','Weight'] else 'normalized_biovolume_std'}).to_list()
        columns_to_save_1b=list(np.insert(pd.Series(df_products_1b.columns.drop(['n_instruments','merged_instruments','Validation_percentage']).astype(str)).values,[0,0,8],['n_instruments','merged_instruments','Validation_percentage'])) if product=='Size' else list(np.insert(pd.Series(df_products_1b.columns.drop(['PSD_slope_mean','PSD_slope_std','PSD_intercept_mean','PSD_intercept_std','PSD_r2_mean','PSD_r2_std','n_instruments','merged_instruments','Validation_percentage']).astype(str)).values,[0,0,8],['n_instruments','merged_instruments','Validation_percentage'])) #pd.Series(df_products_1a.columns.drop(['equivalent_circular_diameter_mean','normalized_abundance_mean','normalized_abundance_std']).astype(str)).replace({'biovolume_size_class':dict_products_x[product] if product in ['Biomass','Weight'] else 'biovolume_size_class','normalized_biovolume_mean':'normalized_biomass_mean' if product in ['Biomass','Weight'] else 'normalized_biomass_means','normalized_biovolume_std':'normalized_biomass_std' if product in ['Biomass','Weight'] else 'normalized_biovolume_std'}).to_list()

        (df_products_1a.rename(columns={'biovolume_size_class':dict_products_x[product] if product in ['Biomass','Weight'] else 'biovolume_size_class','normalized_biovolume_mean':'normalized_biomass_mean' if product in ['Biomass','Weight'] else 'normalized_biomass_means','normalized_biovolume_std':'normalized_biomass_std' if product in ['Biomass','Weight'] else 'normalized_biovolume_std'}))[columns_to_save_1a].to_csv(path_to_directory / str('Merged_multi-instruments_1a_{}-distribution_v'.format(product) + currentYear + '-' + currentMonth + '.csv'),index=False)
        df_products_1b[columns_to_save_1b].to_csv(path_to_directory / str('Merged_multi-instruments_1b_{}-spectra-fit_v'.format( product) + currentYear + '-' + currentMonth + '.csv'), index=False)

        #x=df.loc[list(df.groupby(group+['Sample','sizeClasses']).groups.values())[3]]
        df_summary=df.groupby(group+['Sample','Instrument','sizeClasses']).apply(lambda x:pd.Series({**dict(zip((pd.Series(dict_products_y[product])+'_living').to_list(),x.query('PFT!="Detritus"')[dict_products_y[product]].sum(axis=0))),**dict(zip((pd.Series(dict_products_y[product])+'_nonliving').to_list(),x.query('PFT=="Detritus"')[dict_products_y[product]].sum(axis=0)))})).reset_index()#.apply(lambda x:pd.Series({**dict(zip((pd.Series(dict_products_y[product])+'_living').to_list(),x.query('PFT!="Detritus"')[dict_products_y[product]].sum(axis=0))),**dict(zip((pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_living').to_list(),((x.query('PFT!="Detritus"')[(pd.Series(dict_products_y[product]).str.replace('_mean','_std')).to_list()]**2).sum(axis=0))**0.5)),**dict(zip((pd.Series(dict_products_y[product])+'_nonliving').to_list(),x.query('PFT=="Detritus"')[dict_products_y[product]].sum(axis=0))),**dict(zip((pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_nonliving').to_list(),((x.query('PFT=="Detritus"')[(pd.Series(dict_products_y[product]).str.replace('_mean','_std')).to_list()]**2).sum(axis=0))**0.5))})).reset_index()
        df_summary=pd.merge(df_summary.assign(merged_instruments=df_summary.Instrument),df_merged[['Sample','min_depth','max_depth','sizeClasses','ECD_mid','biomass_mid']].drop_duplicates(),how='left',on=['Sample','sizeClasses'])
        df_summary=df_summary.dropna(subset=['min_depth', 'max_depth']).sort_values(['Sample', 'Latitude', 'Longitude', 'date_bin', 'min_depth', 'max_depth', 'Group_station_index', 'n_instruments', 'merged_instruments','ECD_mid']).reset_index(drop=True)
        df_summary_thres=df_summary.groupby(['Sample','Latitude','Longitude','date_bin','min_depth','max_depth','Group_station_index','n_instruments','merged_instruments']).apply(lambda x: (threshold_func(binned_data=x.rename(columns=dict(zip((pd.Series(dict_products_y[product])+'_living').to_list(),(pd.Series(dict_products_y[product])).to_list()))).assign(count_uncertainty=0,size_uncertainty=0), empty_bins=3, threshold_count=0.2, threshold_size=0.2))).reset_index(drop=True).rename(columns={'Latitude':'midLatBin','Longitude':'midLonBin','min_depth':'Min_obs_depth','max_depth':'Max_obs_depth'}).astype({'midLatBin':float,'midLonBin':float}).assign(size_class_mid=lambda x:(1/6)*np.pi*(x.ECD_mid**3))
        df_1a_original=df_summary_thres.groupby(['merged_instruments']).apply(lambda x: NBSS_stats_func(df=x.assign(PSD=lambda x:x.PSD if 'PSD' in x.columns else np.nan,size_class_mid=lambda x: x[dict_products_x[product]]), light_parsing=False, bin_loc=1, group_by='yyyymm')).reset_index().drop(columns='level_1')
        df_1b_original=df_summary_thres.astype(dict(zip(['midLatBin','midLonBin','date_bin','Min_obs_depth','Max_obs_depth','n_instruments','merged_instruments'],[str]*7))).groupby(['midLatBin','midLonBin','date_bin','Min_obs_depth','Max_obs_depth','n_instruments','merged_instruments']).apply(lambda x:linear_fit_func(x.astype({'midLatBin':float,'midLonBin':float,'Min_obs_depth':float,'Max_obs_depth':float}).assign(logSize=np.log10(x[dict_products_x[product]]/df_bins.loc[0,dict_products_x[product]]),logNB=np.log10(x[dict_products_y[product][0]]),logECD=np.log10(x.ECD_mid),logPSD=np.log10(x.PSD) if 'PSD' in x.columns else np.nan), light_parsing = False, depth_parsing = False)).reset_index()
        df_1b_original=df_1b_original.groupby(['merged_instruments']).apply(lambda x: stats_linfit_func(df=x, light_parsing = False, bin_loc = 1, group_by = 'yyyymm')).reset_index().drop(columns='level_1')
        df_products_original=pd.merge(df_1a_original,df_1b_original,how='left',on=['merged_instruments','year','month','latitude','longitude','min_depth','max_depth','n'])

        df_all=pd.concat([df_products_merged[list(set(df_products_merged.columns) & set(df_products_original.columns))],df_products_original[list(set(df_products_merged.columns) & set(df_products_original.columns))]],axis=0).sort_values(['latitude','longitude','year','month','min_depth','max_depth','merged_instruments','equivalent_circular_diameter_mean']).reset_index(drop=True)[['latitude','longitude','year','month','min_depth','max_depth','merged_instruments','equivalent_circular_diameter_mean','biovolume_size_class','n','normalized_biovolume_mean','normalized_biovolume_std','normalized_abundance_mean','normalized_abundance_std','NBSS_slope_mean','NBSS_slope_std','NBSS_intercept_mean','NBSS_intercept_std','NBSS_r2_mean','NBSS_r2_std','PSD_slope_mean','PSD_slope_std','PSD_intercept_mean','PSD_intercept_std','PSD_r2_mean','PSD_r2_std','N_bins_min']]#pd.concat([df_merged[list(set(df_merged.columns) & set(df_summary.columns))],df_summary[list(set(df_merged.columns) & set(df_summary.columns))]],axis=0).sort_values(['Sample','min_depth','max_depth','n','merged_instruments','ECD_mid']).reset_index(drop=True)[group+['Sample','min_depth','max_depth','sizeClasses','ECD_mid','biomass_mid']+(pd.Series(dict_products_y[product])+'_living').to_list()+(pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_living').to_list()+(pd.Series(dict_products_y[product])+'_nonliving').to_list()+(pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_nonliving').to_list()]#pd.concat([df_merged[list(set(df_merged.columns) & set(df_summary.columns))],df_summary[list(set(df_merged.columns) & set(df_summary.columns))]],axis=0).sort_values(['Sample','min_depth','max_depth','merged_instruments','ECD_mid']).reset_index(drop=True)[group+['Sample','min_depth','max_depth','sizeClasses','ECD_mid','biomass_mid']+(pd.Series(dict_products_y[product])+'_living').to_list()+(pd.Series(dict_products_y[product])+'_nonliving').to_list()]#pd.concat([df_merged[list(set(df_merged.columns) & set(df_summary.columns))],df_summary[list(set(df_merged.columns) & set(df_summary.columns))]],axis=0).sort_values(['Sample','min_depth','max_depth','n','merged_instruments','ECD_mid']).reset_index(drop=True)[group+['Sample','min_depth','max_depth','sizeClasses','ECD_mid','biomass_mid']+(pd.Series(dict_products_y[product])+'_living').to_list()+(pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_living').to_list()+(pd.Series(dict_products_y[product])+'_nonliving').to_list()+(pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_nonliving').to_list()]
        group = ['Group_station_index','Sample', 'merged_instruments']
        df_all=pd.merge(df_all, df_all.drop_duplicates(subset=['year','month','latitude','longitude','min_depth','max_depth'], ignore_index=True)[['year','month','latitude','longitude','min_depth','max_depth']].reset_index().rename({'index': 'Group_station_index'}, axis='columns'), how='left', on=['year','month','latitude','longitude','min_depth','max_depth'])
        df_all['Sample']=df_all.latitude.astype(str) + '_' + df_all.longitude.astype(str) + "_" + df_all.year.astype( str)+ "_" + df_all.month.astype( str)
        df_all = pd.merge(df_all, df_all.drop_duplicates(subset=group, ignore_index=True)[group].reset_index().rename( {'index': 'Group_index'}, axis='columns'), how='left', on=group)
        df_all=df_all.dropna(subset=['min_depth','max_depth']).reset_index(drop=True) # Some initial observations were dropped since there were above the count/size uncertainty threshold
        df_all['Group_index']=df_all.Group_station_index.astype(str)+"_"+df_all.merged_instruments.astype(str)
        df_all['Sample']=df_all.Sample.astype(str)+"_"+df_all.groupby(['Sample']).merged_instruments.transform(lambda x: x[x.str.len().idxmax()]  )
        df_all['Group_index'] = df_all.Group_index.astype(str) + "_slope" + df_all.NBSS_slope_mean.astype(float).round(2).astype(str) + "_intercept" + df_all.NBSS_intercept_mean.astype(float).round(2).astype(str)
        x_axis='equivalent_circular_diameter_mean' if product=='Size' else 'biovolume_size_class'
        fig = standardization_report_func(df_summary=df_all.query('merged_instruments.str.contains("_")').assign(Sample=lambda x:x.latitude.astype(str)).rename(columns={'latitude':'Latitude','longitude':'Longitude'}).groupby(['Sample', 'Latitude', 'Longitude', 'min_depth', 'max_depth'], dropna=True).apply(lambda x: pd.Series({'Abundance': x['normalized_biovolume_mean'].astype(float).sum(),  # individuals per liter
                                                                                                                                                                                     'Average_diameter': np.nanmean( x['normalized_biovolume_mean'].astype(float)* x.equivalent_circular_diameter_mean.astype(float)),# micrometer
                                                                                                                                                                                     'Std_diameter': np.nanstd( x['normalized_biovolume_mean'].astype(float) * x.equivalent_circular_diameter_mean.astype(float))})).reset_index(),
            df_standardized=pd.DataFrame({}), df_nbss=df_all.dropna(subset=['NBSS_slope_mean']).rename( columns={'normalized_biovolume_mean': 'NBSS', x_axis: 'size_class_mid'}),plot='nbss')
        fig.write_html(Path(path_to_directory/'Merged_product_{}_living_multi-instruments.html'.format(product)))

        # Select only regression statistics to re-create the ESSD paper figures, append Global Oceans and Seas regions and save
        df_stats_summary=df_all.dropna(subset=['NBSS_slope_mean']).rename(columns={'latitude':'Latitude','longitude':'Longitude','NBSS_slope_mean':'slope','NBSS_intercept_mean':'intercept','NBSS_r2_mean':'r2'})[['Latitude','Longitude','Sample','year','month','merged_instruments','Group_index','Group_station_index','slope','intercept','r2']].drop_duplicates()#pd.merge(df_all[['Latitude','Longitude','Sample','year','month','merged_instruments','Group_index','Group_station_index']].drop_duplicates(),df_stats[['slope','intercept','R2']].drop_duplicates(),how='left',left_index=True, right_index=True)#pd.merge(df_all[['Latitude','Longitude','Sample','date_bin','merged_instruments','Group_index','Group_station_index']].drop_duplicates(),df_stats[['slope','intercept','R2']].drop_duplicates(),how='left',left_index=True, right_index=True)#pd.merge(df_all[['Latitude','Longitude','Sample','year','month','merged_instruments','Group_index','Group_station_index']].drop_duplicates(),df_stats[['slope','intercept','R2']].drop_duplicates(),how='left',left_index=True, right_index=True)
        gdf = gpd.GeoDataFrame(df_stats_summary[['Group_station_index', 'Longitude', 'Latitude']].drop_duplicates().dropna(),geometry=gpd.points_from_xy(df_stats_summary[['Group_station_index', 'Longitude', 'Latitude']].drop_duplicates().dropna().Longitude, df_stats_summary[['Group_station_index', 'Longitude', 'Latitude']].drop_duplicates().dropna().Latitude))
        df_stats_summary['Study_area'] = pd.merge(df_stats_summary,gpd.tools.sjoin(gdf, oceans, predicate="within", how='left')[['Group_station_index', 'name']], how='left', on='Group_station_index')['name'].astype(str)
        #df_stats_summary.to_csv(path_to_directory/'Merged_product_{}_stats_living.csv'.format(product),index=False)
        # Plot the selection for each PFT
        df_pivot=df_stats_summary.pivot_table(values=['slope','intercept', 'r2'],columns=['merged_instruments'],index=['Latitude', 'Longitude', 'Sample', 'year','month', 'Group_station_index']).reset_index()
        df_pivot.columns=['_'.join(col) if (type(col) is tuple) & (col[1]!='') else col[0] for col in df_pivot.columns.values]
        # Plot and save slope pairplots
        slope_color = "#528787ab"
        g = sns.pairplot(df_pivot[[column for column in df_pivot.columns if 'slope' in column]].rename(columns={column:' / '.join(column.replace("slope_",'').split("_")) for column in df_pivot.columns if 'slope' in column}),kind='scatter',corner=True,diag_kind='kde',plot_kws=dict(marker="o",size=.3, color=slope_color),diag_kws=dict(color=slope_color))
        g.map_offdiag(seaborn_plot_unity)
        g.map_offdiag(seaborn_r2,color=slope_color,xy=(.05, .95))
        g.map_diag(seaborn_diag_func,color=slope_color)
        g.fig.set_size_inches(10,10)
        g.fig.suptitle(r'Spectral slopes (L$^{-1}$ $\mu$m$^{-3}$)',x=0.5,y=0,size=12,family='serif', color=slope_color)
        for ax in g.axes.flatten():
            if ax:
                ax.tick_params(axis='y', labelrotation=90,labelsize=8,color=slope_color)
        g.savefig(fname='{}/PSSdb_Merged-{}-products_slopes_comparison.svg'.format(str(path_to_directory),product), dpi=300)

        # Plot and save intercept pairplots
        intercept_color="#de8787ab"
        g = sns.pairplot(df_pivot[[column for column in df_pivot.columns if 'intercept' in column]].rename(columns={column:' / '.join(column.replace("intercept_",'').split("_")) for column in df_pivot.columns if 'intercept' in column}),kind='scatter',corner=False,diag_kind='kde',plot_kws=dict(marker="o",size=.3, color=intercept_color),diag_kws=dict(color=intercept_color))
        g.map_offdiag(seaborn_plot_unity)
        g.map_offdiag(seaborn_r2,color=intercept_color,xy=(.05, .05))
        g.map_diag(seaborn_diag_func,color=intercept_color)
        g.map_lower(seaborn_hide_axis)
        for ax in g.axes.flat:
            sns.despine(left=True, right=False, bottom=True, top=False, ax=ax)
            ax.xaxis.set_ticks_position('top')
            ax.yaxis.set_ticks_position('right')
            plt.setp(ax.yaxis.get_ticklabels(), visible=False,rotation=-90)
            plt.setp(ax.xaxis.get_ticklabels(), visible=False)

        for ax1, ax2 in g.axes[:, [0, -1]]:
            ax2.yaxis.set_label_position('right')
            ax2.set_ylabel(ax1.get_ylabel(), visible=True,rotation=-90)
            ax1.set_ylabel('')
            ax2.yaxis.set_ticks_position('right')
            plt.setp(ax2.yaxis.get_ticklabels(), visible=True, rotation=-90,color=intercept_color)
        for ax1, ax2 in g.axes[[0, -1], :].T:
            ax1.xaxis.set_label_position('top')
            ax1.set_xlabel(ax2.get_xlabel(), visible=True)
            ax2.set_xlabel('')
            ax1.xaxis.set_ticks_position('top')
            plt.setp(ax1.xaxis.get_ticklabels(), visible=True,color=intercept_color)
        g.fig.set_size_inches(10, 10)
        g.fig.suptitle(r'log$_{10}$ Spectral intercepts ($\mu$m$^{3}$ L$^{-1}$ $\mu$m$^{-3}$)', x=0.5, y=1.2, size=12, family='serif', color=intercept_color)
        g.savefig(fname='{}/PSSdb_Merged-{}-products_intercepts_comparison.svg'.format(str(path_to_directory),product), dpi=300)

        # Now work with df_single to generate products for bins with only one instrument recorded. Products will still be different from the bulk releases since here we generate "only-living" size spectra
        bar.set_description("Merging taxa-specific {} products. Generating the products for single-instrument bins".format(product), refresh=True)

        #x=df_single.loc[list(df_single.groupby(['Latitude', 'Longitude', 'date_bin','Group_station_index','n_instruments','merged_instruments','sizeClasses']).groups.values())[0]]
        group=['Latitude', 'Longitude', 'date_bin','Group_station_index','n_instruments','merged_instruments']
        df_merged_single=df_single.groupby(['Latitude', 'Longitude', 'date_bin','Group_station_index','n_instruments','merged_instruments','sizeClasses']).apply(lambda x:pd.Series({**{'Validation_percentage':np.nanmean(x.Validation_percentage),'min_depth':df_single[df_single[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].min_depth.min(),'max_depth':df_single[df_single[group].apply(tuple, axis=1).isin(x[group].apply(tuple, axis=1))].max_depth.max()},**dict(zip((pd.Series(dict_products_y[product])+'_living').to_list(),x.query('PFT!="Detritus"')[dict_products_y[product]].sum(axis=0))),**dict(zip((pd.Series(dict_products_y[product])+'_nonliving').to_list(),x.query('PFT=="Detritus"')[dict_products_y[product]].sum(axis=0)))})).reset_index()#df_selection.groupby(['Latitude', 'Longitude', 'year', 'month','min_depth','max_depth','Group_station_index','n_instruments','merged_instruments','sizeClasses']).apply(lambda x:pd.Series({**{'n':x.n.min()},**dict(zip((pd.Series(dict_products_y[product])+'_living').to_list(),x.query('PFT!="Detritus"')[dict_products_y[product]].sum(axis=0))),**dict(zip((pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_living').to_list(),((x.query('PFT!="Detritus"')[(pd.Series(dict_products_y[product]).str.replace('_mean','_std')).to_list()]**2).sum(axis=0))**0.5)),**dict(zip((pd.Series(dict_products_y[product])+'_nonliving').to_list(),x.query('PFT=="Detritus"')[dict_products_y[product]].sum(axis=0))),**dict(zip((pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_nonliving').to_list(),((x.query('PFT=="Detritus"')[(pd.Series(dict_products_y[product]).str.replace('_mean','_std')).to_list()]**2).sum(axis=0))**0.5))})).reset_index()
        df_merged_single=pd.merge(df_merged_single,df_bins.drop(columns=['sizeClasses']).assign(sizeClasses=pd.cut(data_bins[dict_products_x[product]].to_numpy(),data_bins[dict_products_x[product]].to_numpy()).categories.values.astype(str)),how='left',on='sizeClasses').sort_values(['Latitude', 'Longitude', 'date_bin','min_depth','max_depth','ECD_mid']).reset_index(drop=True)

        # Check merged products along with original data after generating final product
        df_merged_single['Sample']=df_merged_single.Latitude.astype(str) + '_' + df_merged_single.Longitude.astype(str) + "_" + df_merged_single.date_bin.astype( str)

        bar.set_description("Merging taxa-specific {} products. Saving all merged single-instrument sub-bin products to {}".format(product,str(path_to_directory).replace(str(Path.home()),'~')), refresh=True)
        df_merged_single.to_csv(path_to_directory / str('Merged_single-instruments_{}-distribution_all_var_v'.format(product) + currentYear + '-' + currentMonth + '.csv'),index=False)

        ## first, apply thresholding function, then average at bin level
        df_merged_single_thres=df_merged_single.groupby(['Sample','Latitude','Longitude','date_bin','min_depth','max_depth','Group_station_index','n_instruments','merged_instruments']).apply(lambda x: (threshold_func(binned_data=x.rename(columns=dict(zip((pd.Series(dict_products_y[product])+'_living').to_list(),(pd.Series(dict_products_y[product])).to_list()))).assign(count_uncertainty=0,size_uncertainty=0), empty_bins=3, threshold_count=0.2, threshold_size=0.2))).reset_index(drop=True)
        df_products_1a_single=NBSS_stats_func(df=df_merged_single_thres.rename(columns={'Latitude':'midLatBin','Longitude':'midLonBin','min_depth':'Min_obs_depth','max_depth':'Max_obs_depth'}).astype({'midLatBin':float,'midLonBin':float}).assign(PSD=lambda x:x.PSD if 'PSD' in x.columns else np.nan,size_class_mid=lambda x: x[dict_products_x[product]]), light_parsing=False, bin_loc=1, group_by='yyyymm')
        df_products_1a_single=pd.merge(df_products_1a_single,df_merged_single_thres.assign(biovolume_size_class=lambda x: x[dict_products_x[product]],range_biomass_bin=lambda x:x.range_biomass_bin,latitude=lambda x:pd.cut(x.Latitude.astype(float),bins=np.arange(-90,91,1),labels=np.arange(-90,91,0.5)[1:-1:2]).astype(str),longitude=lambda x:pd.cut(x.Longitude.astype(float),bins=np.arange(-180,181,1),labels=np.arange(-180,181,0.5)[1:-1:2]).astype(str),year=lambda x:(x.date_bin.str[0:4]).astype(str),month=lambda x:(x.date_bin.str.split("_").str[-2]).astype(str)).astype(dict(zip(['latitude','longitude','year','month','biovolume_size_class','range_biomass_bin','min_depth','max_depth','n_instruments','merged_instruments'],[str]*10))).groupby(['latitude','longitude','year','month','biovolume_size_class','range_biomass_bin','min_depth','max_depth']).apply(lambda x: pd.Series({'n_instruments':len(x.merged_instruments.unique()[0].split("_")),'merged_instruments':'_'.join(natsorted(x.merged_instruments.unique()[0].split("_"))),'Validation_percentage':x.Validation_percentage.mean()})).reset_index().astype((df_products_1a.dtypes.loc[['latitude','longitude','year','month','min_depth','max_depth','biovolume_size_class']]).to_dict()),how='left',on=['latitude','longitude','year','month','min_depth','max_depth','biovolume_size_class'])
        df_products_1b_single=df_merged_single_thres.rename(columns={'Latitude':'midLatBin','Longitude':'midLonBin','min_depth':'Min_obs_depth','max_depth':'Max_obs_depth'}).astype(dict(zip(['midLatBin','midLonBin','date_bin','Min_obs_depth','Max_obs_depth','n_instruments','merged_instruments'],[str]*7))).groupby(['midLatBin','midLonBin','date_bin','Min_obs_depth','Max_obs_depth','n_instruments','merged_instruments']).apply(lambda x:linear_fit_func(x.astype({'midLatBin':float,'midLonBin':float,'Min_obs_depth':float,'Max_obs_depth':float}).assign(logSize=np.log10(x[dict_products_x[product]]/df_bins.loc[0,dict_products_x[product]]),logNB=np.log10(x[dict_products_y[product][0]]),logECD=np.log10(x.ECD_mid),logPSD=np.log10(x.PSD) if 'PSD' in x.columns else np.nan), light_parsing = False, depth_parsing = False).assign(Validation_percentage=np.nanmean(x.Validation_percentage))).reset_index()
        df_products_1b_single=pd.merge(stats_linfit_func(df=df_products_1b_single, light_parsing = False, bin_loc = 1, group_by = 'yyyymm'),df_products_1b_single.assign(latitude=lambda x:pd.cut(x.midLatBin.astype(float),bins=np.arange(-90,91,1),labels=np.arange(-90,91,0.5)[1:-1:2]).astype(str),longitude=lambda x:pd.cut(x.midLonBin.astype(float),bins=np.arange(-180,181,1),labels=np.arange(-180,181,0.5)[1:-1:2]).astype(str),year=lambda x:(x.date_bin.str[0:4]).astype(str),month=lambda x:(x.date_bin.str.split("_").str[-1]).astype(str)).astype(dict(zip(['latitude','longitude','year','month','Min_obs_depth','Max_obs_depth'],[str]*6))).groupby(['latitude','longitude','year','month','Min_obs_depth','Max_obs_depth']).apply(lambda x: pd.Series({'n_instruments':len(x.merged_instruments.unique()[0].split("_")),'merged_instruments':'_'.join(natsorted(x.merged_instruments.unique()[0].split("_"))),'Validation_percentage':np.nanmean(x.Validation_percentage)})).reset_index().rename(columns={'Min_obs_depth':'min_depth','Max_obs_depth':'max_depth'}).astype({'latitude':float,'longitude':float,'year':str,'month':str,'min_depth':float,'max_depth':float}),how='left',on=['latitude','longitude','year','month','min_depth','max_depth'])
        df_products_merged_single=pd.merge(df_products_1a_single,df_products_1b_single,how='left',on=['merged_instruments','year','month','latitude','longitude','min_depth','max_depth','n'])
        df_products_1a_single, df_products_1b_single =ocean_label_func(df=df_products_1a_single, Lon='longitude', Lat='latitude'),ocean_label_func(df=df_products_1b_single, Lon='longitude', Lat='latitude')
        df_meta_1a_single=df_products_1a_single[['range_biomass_bin','n_instruments','merged_instruments','Validation_percentage']].copy()
        df_meta_1b_single=df_products_1b_single[['n_instruments','merged_instruments','Validation_percentage']].copy()

        df_products_1a_single,df_products_1b_single=QC_products(df_products_1a_single,df_products_1b_single, grouping_factors=['year', 'month', 'latitude', 'longitude', 'ocean'])
        df_products_1a_single,df_products_1b_single=pd.concat([df_products_1a_single,df_meta_1a_single],axis=1,ignore_index=False),pd.concat([df_products_1b_single,df_meta_1b_single],axis=1,ignore_index=False)

        bar.set_description( "Merging taxa-specific {} products. Saving merged single-instrument products 1a/b to {}".format(product,str(path_to_directory).replace(str(Path.home()),'~')), refresh = True)
        columns_to_save_1a=list(np.insert(pd.Series(df_products_1a_single.columns.drop(['n_instruments','merged_instruments','Validation_percentage']).astype(str)).values,[0,0,9],['n_instruments','merged_instruments','Validation_percentage'])) if product=='Size' else list(np.insert(pd.Series(df_products_1a_single.columns.drop(['equivalent_circular_diameter_mean','normalized_abundance_mean','normalized_abundance_std','range_biomass_bin','n_instruments','merged_instruments','Validation_percentage']).astype(str)).replace({'biovolume_size_class':dict_products_x[product] if product in ['Biomass','Weight'] else 'biovolume_size_class','normalized_biovolume_mean':'normalized_biomass_mean' if product in ['Biomass','Weight'] else 'normalized_biomass_means','normalized_biovolume_std':'normalized_biomass_std' if product in ['Biomass','Weight'] else 'normalized_biovolume_std'}).values,[0,0,9,9],['n_instruments','merged_instruments','range_biomass_bin','Validation_percentage'])) #pd.Series(df_products_1a.columns.drop(['equivalent_circular_diameter_mean','normalized_abundance_mean','normalized_abundance_std']).astype(str)).replace({'biovolume_size_class':dict_products_x[product] if product in ['Biomass','Weight'] else 'biovolume_size_class','normalized_biovolume_mean':'normalized_biomass_mean' if product in ['Biomass','Weight'] else 'normalized_biomass_means','normalized_biovolume_std':'normalized_biomass_std' if product in ['Biomass','Weight'] else 'normalized_biovolume_std'}).to_list()
        columns_to_save_1b=list(np.insert(pd.Series(df_products_1b_single.columns.drop(['n_instruments','merged_instruments','Validation_percentage']).astype(str)).values,[0,0,8],['n_instruments','merged_instruments','Validation_percentage'])) if product=='Size' else list(np.insert(pd.Series(df_products_1b_single.columns.drop(['PSD_slope_mean','PSD_slope_std','PSD_intercept_mean','PSD_intercept_std','PSD_r2_mean','PSD_r2_std','n_instruments','merged_instruments','Validation_percentage']).astype(str)).values,[0,0,8],['n_instruments','merged_instruments','Validation_percentage'])) #pd.Series(df_products_1a.columns.drop(['equivalent_circular_diameter_mean','normalized_abundance_mean','normalized_abundance_std']).astype(str)).replace({'biovolume_size_class':dict_products_x[product] if product in ['Biomass','Weight'] else 'biovolume_size_class','normalized_biovolume_mean':'normalized_biomass_mean' if product in ['Biomass','Weight'] else 'normalized_biomass_means','normalized_biovolume_std':'normalized_biomass_std' if product in ['Biomass','Weight'] else 'normalized_biovolume_std'}).to_list()

        (df_products_1a_single.rename(columns={'biovolume_size_class':dict_products_x[product] if product in ['Biomass','Weight'] else 'biovolume_size_class','normalized_biovolume_mean':'normalized_biomass_mean' if product in ['Biomass','Weight'] else 'normalized_biomass_means','normalized_biovolume_std':'normalized_biomass_std' if product in ['Biomass','Weight'] else 'normalized_biovolume_std'}))[columns_to_save_1a].to_csv(path_to_directory / str('Merged_single-instrument_1a_{}-distribution_v'.format(product) + currentYear + '-' + currentMonth + '.csv'),index=False)
        df_products_1b_single[columns_to_save_1b].to_csv(path_to_directory / str('Merged_single-instrument_1b_{}-spectra-fit_v'.format( product) + currentYear + '-' + currentMonth + '.csv'), index=False)
        # Generate interactive plots
        df_all=df_products_merged_single.sort_values(['latitude','longitude','year','month','min_depth','max_depth','merged_instruments','equivalent_circular_diameter_mean']).reset_index(drop=True)[['latitude','longitude','year','month','min_depth','max_depth','merged_instruments','equivalent_circular_diameter_mean','biovolume_size_class','n','normalized_biovolume_mean','normalized_biovolume_std','normalized_abundance_mean','normalized_abundance_std','NBSS_slope_mean','NBSS_slope_std','NBSS_intercept_mean','NBSS_intercept_std','NBSS_r2_mean','NBSS_r2_std','PSD_slope_mean','PSD_slope_std','PSD_intercept_mean','PSD_intercept_std','PSD_r2_mean','PSD_r2_std','N_bins_min']]#pd.concat([df_merged[list(set(df_merged.columns) & set(df_summary.columns))],df_summary[list(set(df_merged.columns) & set(df_summary.columns))]],axis=0).sort_values(['Sample','min_depth','max_depth','n','merged_instruments','ECD_mid']).reset_index(drop=True)[group+['Sample','min_depth','max_depth','sizeClasses','ECD_mid','biomass_mid']+(pd.Series(dict_products_y[product])+'_living').to_list()+(pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_living').to_list()+(pd.Series(dict_products_y[product])+'_nonliving').to_list()+(pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_nonliving').to_list()]#pd.concat([df_merged[list(set(df_merged.columns) & set(df_summary.columns))],df_summary[list(set(df_merged.columns) & set(df_summary.columns))]],axis=0).sort_values(['Sample','min_depth','max_depth','merged_instruments','ECD_mid']).reset_index(drop=True)[group+['Sample','min_depth','max_depth','sizeClasses','ECD_mid','biomass_mid']+(pd.Series(dict_products_y[product])+'_living').to_list()+(pd.Series(dict_products_y[product])+'_nonliving').to_list()]#pd.concat([df_merged[list(set(df_merged.columns) & set(df_summary.columns))],df_summary[list(set(df_merged.columns) & set(df_summary.columns))]],axis=0).sort_values(['Sample','min_depth','max_depth','n','merged_instruments','ECD_mid']).reset_index(drop=True)[group+['Sample','min_depth','max_depth','sizeClasses','ECD_mid','biomass_mid']+(pd.Series(dict_products_y[product])+'_living').to_list()+(pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_living').to_list()+(pd.Series(dict_products_y[product])+'_nonliving').to_list()+(pd.Series(dict_products_y[product]).str.replace('_mean','_std')+'_nonliving').to_list()]
        group = ['Group_station_index','Sample', 'merged_instruments']
        df_all=pd.merge(df_all, df_all.drop_duplicates(subset=['year','month','latitude','longitude','min_depth','max_depth'], ignore_index=True)[['year','month','latitude','longitude','min_depth','max_depth']].reset_index().rename({'index': 'Group_station_index'}, axis='columns'), how='left', on=['year','month','latitude','longitude','min_depth','max_depth'])
        df_all['Sample']=df_all.latitude.astype(str) + '_' + df_all.longitude.astype(str) + "_" + df_all.year.astype( str)+ "_" + df_all.month.astype( str)
        df_all = pd.merge(df_all, df_all.drop_duplicates(subset=group, ignore_index=True)[group].reset_index().rename( {'index': 'Group_index'}, axis='columns'), how='left', on=group)
        df_all=df_all.dropna(subset=['min_depth','max_depth']).reset_index(drop=True) # Some initial observations were dropped since there were above the count/size uncertainty threshold
        df_all['Group_index']=df_all.Group_station_index.astype(str)+"_"+df_all.merged_instruments.astype(str)
        df_all['Sample']=df_all.Sample.astype(str)+"_"+df_all.groupby(['Sample']).merged_instruments.transform(lambda x: x[x.str.len().idxmax()]  )
        df_all['Group_index'] = df_all.Group_index.astype(str) + "_slope" + df_all.NBSS_slope_mean.astype(float).round(2).astype(str) + "_intercept" + df_all.NBSS_intercept_mean.astype(float).round(2).astype(str)
        x_axis='equivalent_circular_diameter_mean' if product=='Size' else 'biovolume_size_class'
        fig = standardization_report_func(df_summary=df_all.query('merged_instruments.str.contains("_")').assign(Sample=lambda x:x.latitude.astype(str)).rename(columns={'latitude':'Latitude','longitude':'Longitude'}).groupby(['Sample', 'Latitude', 'Longitude', 'min_depth', 'max_depth'], dropna=True).apply(lambda x: pd.Series({'Abundance': x['normalized_biovolume_mean'].astype(float).sum(),  # individuals per liter
                                                                                                                                                                                     'Average_diameter': np.nanmean( x['normalized_biovolume_mean'].astype(float)* x.equivalent_circular_diameter_mean.astype(float)),# micrometer
                                                                                                                                                                                     'Std_diameter': np.nanstd( x['normalized_biovolume_mean'].astype(float) * x.equivalent_circular_diameter_mean.astype(float))})).reset_index(),
            df_standardized=pd.DataFrame({}), df_nbss=df_all.dropna(subset=['NBSS_slope_mean']).rename( columns={'normalized_biovolume_mean': 'NBSS', x_axis: 'size_class_mid'}),plot='nbss')
        fig.write_html(Path(path_to_directory/'Merged_product_{}_living_single-instrument.html'.format(product)))


        # Update progress bar and move to next product
        ok = bar.update(n=1)

print("\nProcessed finished. Merged products should be uploaded to Zenodo under https://zenodo.org/records/xxxx with a new record")